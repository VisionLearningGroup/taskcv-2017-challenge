---
layout: default
---

            
<section style="text-align: justify">
	<h1> Visual Domain Adaptation Challenge </h1>
</br></br>

<h2 class="section-title"> News </h2>
<div class="summary">
	<p> We will be releasing the data and devkits on June 19th, at 22:00 eastern US time. Stay tuned!</p>

<h2 class="section-title"> Overview </h2>
<div class="summary">
<p> We are pleased to announce the 2017 Visual Domain Adaptation (VisDA2017) Challenge! It is well known that the success of machine learning methods on visual recognition tasks is highly dependent on access to large labeled datasets. Unfortunately, performance often drops significantly when the model is presented with data from a new deployment domain which it did not see in training, a problem known as <i>dataset shift</i>. The VisDA challenge aims to test domain adaptation methods’ ability to transfer source knowledge and adapt it to novel target domains. </p>

</br>

<figure>
<img class="img-responsive" src="{{site.baseurl}}/assets/images/bad-example.png"/>
</figure>
<p><strong>Caption:</strong> An example of a deep learning model failing to properly segment the scene into semantic categories such as road (green), building (gray), etc., because the input image looks different from its training data. Unsupervised Domain Adaptation methods aim to use labeled samples from the train domain and large volumes of unlabeled samples from the test domain to reduce a prediction error on the test domain. [1]</p>
</br>
<h2 class="section-title"> Timeline </h2>

<p> The competition will take place during the months of June -- September 2017<!-- (see <a href="#dates"> dates </a>) -->, and the top performing teams will be invited to present their results at the <a href="http://adas.cvc.uab.es/task-cv2017/">TASK-CV</a> workshop at <a href="http://iccv2017.thecvf.com/">ICCV 2017 in Venice, Italy</a>. This year’s challenge focuses on  synthetic-to-real visual domain shifts and includes two tracks: </p>

  <ul>
    <li>image classification</li>
    <li>image segmentation</li>
  </ul>

<p> Participants are welcome to enter in one or both tracks. </p>
</br>

<h2 class="section-title"> Classification Track </h2>
<p> In this challenge, the goal is to develop a method of unsupervised domain adaptation for image classification. Participants will be given three datasets, each containing the same object categories: </p> 
  <ul>
    <li> <b> training data (source)</b>: synthetic 2D renderings of 3D models generated from different angles and with different lighting conditions, </li>
    <li> <b> validation data (target)</b>: a photo-realistic or real-image validation domain that participants can use to evaluate performance of their domain adaptation methods, and </li>
    <li> <b> test data (target)</b>: a new real-image test domain, <b> different from the validation domain and without labels </b>. The test set will be released shortly before the end of the competition. </li>
  </ul>
<p> The reason for using different target domains for validation and test is to evaluate the performance of proposed models as an out-of-the-box domain adaptation tool. This setting more closely mimics realistic deployment scenarios where the target domain is unknown at training time and discourages algorithms that are designed to handle a particular target domain. </p>

<figure>
<img class="img-responsive" src="{{site.baseurl}}/assets/images/classification-shift.png"/>
</figure>

</br>

<h2 class="section-title"> Segmentation Track </h2>

<p> In this challenge, the goal is to develop an algorithm that can adapt between synthetic dashcam views and real dashcam footage for the semantic image segmentation task. The training data will include pixel-level semantic annotations for 19 classes. We will also provide validation and testing data, following same protocol: </p>
<ul>
    <li> <b>training data (source)</b>: synthetic dashcam renderings along with semantic segmentation labels for 19 classes,</li>
    <li> <b>validation data (target)</b>: a real world collection of dashcam images along with semantic segmentation labels for the corresponding 19 classes to be used for validating the unsupervised semantic segmentation performance, and</li>
    <li> <b>test data (target)</b>: a collection of real world dashcam images, different from the validation domain and without labels. The test set will be released shortly before the end of the competition. </li>
  </ul>
<figure>
<img class="img-responsive" src="{{site.baseurl}}/assets/images/semantic-shift.png" style="display: block; margin: auto;"/>
</figure>

</br> 

<h2 class="section-title"> Dates and Registration </h2>
<p> 
	Please, use the <a href="#reg">registration form</a> on the top in order to receive an invitation link after the challenge is officially launched.
<ul>
<li>Registration Starts: May 1st</li>
<li>Challenge Starts: June 19th</li>
<li>Final submission: September 29th</li>
<li>Winners notification: October 13th</li>
</ul>
</p>

</br>

<h2 class="section-title"> Organizers </h2>

Kate Saenko (Boston University), 
Ben Usman (Boston University),
Xingchao Peng (Boston University),
Neela Kaushik (Boston University),
Judy Hoffman (Stanford University),
Dequan Wang (UC Berkeley)


</br></br></br></br></br>
[1] FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation, Judy Hoffman, Dequan Wang, Fisher Yu, Trevor Darrell

</br>

</div><!--//summary-->
</section><!--//section-->

<div style="height:700px; width:100%; clear:both;"></div>
