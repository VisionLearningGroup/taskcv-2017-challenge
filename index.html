---
layout: default
---

            
<section style="text-align: justify">
	<h1> Visual Domain Adaptation Challenge </h1>
</br></br>
<h2 class="section-title"> Overview </h2>
<div class="summary">
<p> We are pleased to announce the 2017 Visual Domain Adaptation (VisDA) Challenge! It is well known that the success of machine learning methods on visual recognition tasks is highly dependent on access to large labeled datasets. Unfortunately, performance often drops significantly when the model is presented with data from a new deployment domain which it did not see in training, a problem known as <i>dataset shift</i>. The VisDA challenge aims to test domain adaptation methods’ ability to transfer source knowledge and adapt it to novel target domains. </p>

</br>

<figure>
<img class="img-responsive" src="{{site.baseurl}}/assets/images/bad-example.png"/>
</figure>
<p><strong>Caption:</strong> If a domain shift between train and test domains is large, a segmentation model trained on source domain fails on samples from test domain. Unsupervised Domain Adaptation methods aim to use labeled samples from train domain and large volumes of unlabeled samples from the test domain to reduce risk on it. [1]</p>

</br>
<h2 class="section-title"> Timeline </h2>

<p> The competition will take place during the months of June -- September 2017<!-- (see <a href="#dates"> dates </a>) -->, and the top performing teams will be invited to present their results at the <a href="http://adas.cvc.uab.es/task-cv2017/">TASK-CV</a> workshop at <a href="http://iccv2017.thecvf.com/">ICCV 2017 in Venice, Italy</a>. This year’s challenge focuses on  synthetic-to-real visual domain shifts and includes two tracks: </p>

  <ul>
    <li>image classification</li>
    <li>image segmentation</li>
  </ul>

<p> Participants are welcome to enter in one or both tracks. </p>
</br>

<h2 class="section-title"> Classification Track </h2>
<p> In this challenge, the goal is to develop a method of unsupervised domain adaptation for image classification. Participants will be given three datasets, each containing the same object categories: </p> 
  <ul>
    <li> <b> training data (source)</b>: synthetic 2D renderings of 3D models generated from different angles and with different lighting conditions, </li>
    <li> <b> validation data (target)</b>: a photo-realistic or real-image validation domain that participants can use to evaluate performance of their domain adaptation methods, and </li>
    <li> <b> test data (target)</b>: a new real-image test domain, <b> different from the validation domain and without labels </b>. The test set will be released shortly before the end of the competition. </li>
  </ul>
<p> The reason for using different target domains for validation and test is to evaluate the performance of proposed models as an out-of-the-box domain adaptation tool. This setting more closely mimics realistic deployment scenarios where the target domain is unknown at training time and discourages algorithms that are designed to handle to a particular target domain. </p>

<figure>
<img class="img-responsive" src="{{site.baseurl}}/assets/images/classification-shift.png"/>
</figure>

</br>

<h2 class="section-title"> Segmentation Track </h2>

<p> In this challenge, the goal is to develop an algorithm that can adapt between synthetic dashcam views and real dashcam footage for the semantic image segmentation task. The training data will include pixel-level semantic annotations for 13 classes. We will also provide validation and testing data, the same way as described above. </p>

<figure>
<img class="img-responsive" src="{{site.baseurl}}/assets/images/semantic-shift.png" style="display: block; margin: auto;"/>
</figure>

</br> 

<h2 class="section-title"> Dates and Registration </h2>
<p> 
	Please, use the <a href="#reg">registration form</a> on the top in order to receive invitation link after the challenge is officially launched.
<ul>
<li>Registration Starts: May 1st</li>
<li>Challenge Starts: June 19th</li>
<li>Final submission: September 29th</li>
<li>Winners notification: October 13th</li>
</ul>
</p>

</br>

<h2 class="section-title"> Organizers </h2>

Kate Saenko (Boston University), 
Ben Usman (Boston University),
Xingchao Peng (Boston University),
Neela Kaushik (Boston University),
Judy Hoffman (Stanford University),
Dequan Wang (UC Berkeley)


</br></br></br></br></br>
[1] FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation, Judy Hoffman, Dequan Wang, Fisher Yu, Trevor Darrell

</br>

</div><!--//summary-->
</section><!--//section-->

<div style="height:700px; width:100%; clear:both;"></div>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-98380184-1', 'auto');
  ga('send', 'pageview');

</script>